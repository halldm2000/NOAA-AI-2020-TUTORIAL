{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_v0.9",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halldm2000/NOAA-AI-2020-TUTORIAL/blob/master/04_Classification/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJmIVn9jx8O7"
      },
      "source": [
        "### Classify Tropical Cyclones by Category\n",
        "\n",
        "**Download data from a shared google drive** (~2 min)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcPyFA2TyA2I"
      },
      "source": [
        "%%time\n",
        "!gdown https://drive.google.com/uc?id=1-CG3sF1JIi_PGDo6stwwmXiTFJuEUqfU\n",
        "!echo \"Extracting...\"\n",
        "!tar -xzf cyclones.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY6YdMFe4gIQ"
      },
      "source": [
        "**Ensure NetCDF4 is installed, so we can read the data from a file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCaAdaJD3TwY"
      },
      "source": [
        "!pip install netcdf4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDZ2_21Q6imW"
      },
      "source": [
        "**Plot some data to make sure things are working correctly**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V64Nbo_a1iWA"
      },
      "source": [
        "import glob, xarray, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "files = glob.glob(\"crop_64/CAT2/*.nc\");\n",
        "for file in files[:1]:\n",
        "\n",
        "  # open netcdf file usiing xarray\n",
        "  print(\"file =\",file)\n",
        "  ds   = xarray.open_dataset(file);\n",
        "  wind = np.sqrt(ds[\"u\"].data**2 + ds[\"v\"].data**2) # compute wind speed from u,v\n",
        "\n",
        "  # plot each of the fields \n",
        "  plt.figure(figsize=(15,5),dpi=72)\n",
        "  plt.subplot(1,6,1); plt.imshow(ds[\"pwat\"].data, cmap=\"bone\");    plt.axis('off'); plt.title(\"precipitable water\")\n",
        "  plt.subplot(1,6,2); plt.imshow(wind,            cmap=\"jet\");     plt.axis('off'); plt.title(\"wind speed\")\n",
        "  plt.subplot(1,6,3); plt.imshow(ds[\"mslp\"].data, cmap=\"plasma\");  plt.axis('off'); plt.title(\"mean sea level pressure\")\n",
        "  plt.subplot(1,6,4); plt.imshow(ds[\"rh\"].data,   cmap=\"viridis\"); plt.axis('off'); plt.title(\"relative humidity\")\n",
        "  plt.subplot(1,6,5); plt.imshow(ds[\"lat\"].data,  cmap=\"gray\");    plt.axis('off'); plt.title(\"latitude\")\n",
        "  plt.subplot(1,6,6); plt.imshow(ds[\"lon\"].data,  cmap=\"gray\");    plt.axis('off'); plt.title(\"longitude\")\n",
        "  plt.subplots_adjust(wspace=0.05, hspace=0)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSD0celNM785"
      },
      "source": [
        "**Initialize GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqmHDa2Wb9bj"
      },
      "source": [
        "import torch, nvidia_smi\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device = {device}\")\n",
        "\n",
        "if(device.type ==\"cuda\"):\n",
        "  nvidia_smi.nvmlInit()\n",
        "  handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "  print(\"GPU type =\",torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faU9Ys1AvrqQ"
      },
      "source": [
        "**Define utility functions to facilitate training** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr-caD58m5Bt"
      },
      "source": [
        "import torch, os, psutil, pathlib, xarray as xr\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "# Dataset: custom set of x,y pairs for Tropical Cyclone classification\n",
        "\n",
        "class TropicalCycloneDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  This dataset maps GFS fields as input to cyclone categories as output, for classification. \n",
        "  The inputs are read from netcdf files using the given \"load function\" \n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, source, fields=None, categories=None, load_fcn=None, start=0, stop=1):\n",
        "\n",
        "    # get input and out fields \n",
        "    self.inputs  = [\"lat\",\"lon\",\"pwat\",\"mslp\",\"u\",\"v\",\"rh\"] if (fields==None) else fields         \n",
        "    self.outputs = ['TD','TS','CAT1','CAT2','CAT3','CAT4','CAT5','DS','ET','SS','NONE'] if (categories==None) else categories\n",
        "    self.load_fcn = load_fcn\n",
        "\n",
        "    # append a list of filenames for each output category\n",
        "    self.files, self.categories = [],[]\n",
        "    path = pathlib.Path(source)\n",
        "    for i,c in enumerate(self.outputs):\n",
        "\n",
        "      files = sorted(list((path/c).glob('**/*.nc')))\n",
        "      lo,hi = int(start*len(files)), int(stop*len(files))\n",
        "      files = files[lo:hi]\n",
        "      self.files.extend(files)\n",
        "      self.categories.extend([i]*len(files))\n",
        "      print(f\"Category {c} has {len(files)} items\")\n",
        "\n",
        "    print(f\"TCDataset: inputs={self.inputs} outputs={self.outputs} num examples = {len(self.files)}\\n\")\n",
        "    \n",
        "  def __len__(self): \n",
        "    return len(self.files)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.load_fcn(self.files[index], self.inputs)\n",
        "    y = torch.tensor(self.categories[index])\n",
        "\n",
        "    return x,y\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "# Loading function for reading NetCDF files\n",
        "\n",
        "def load_netcdf(file, fields, device=\"cpu\"):\n",
        "  \"\"\"\n",
        "  This function loads data from a netcdf file \n",
        "  and stores it iin a torch tensor on the cpu or gpu\n",
        "  \"\"\"\n",
        "  ds   = xr.open_dataset(file) \n",
        "  x    = torch.zeros(len(fields), 64, 64, device=device)\n",
        "\n",
        "  for i,field in enumerate(fields):\n",
        "    x[i,:,:] = torch.tensor(ds[field].data)\n",
        "    if(field==\"lat\"): x[i,:,:]/=90.0\n",
        "    if(field==\"lon\"): x[i,:,:]/=360.0\n",
        "  return x\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "# Data Cache for speeding up data access\n",
        "\n",
        "class Cache():\n",
        "    \"\"\"\n",
        "    This class loads data from RAM if it is available, and reads it from disk\n",
        "    if it is not. This avoids multiple disk access for the same data.\n",
        "    \"\"\"\n",
        "    def __init__(self, load_fcn, device=\"cpu\", memory_cap = 80):\n",
        "        self.fcn   = load_fcn\n",
        "        self.cap   = memory_cap\n",
        "        self.device= device\n",
        "        self.cache = {}\n",
        "        \n",
        "    def load(self, path, fields):\n",
        "        \n",
        "        if path in self.cache: return self.cache[path]\n",
        "        if (psutil.virtual_memory().percent < self.cap):\n",
        "            self.cache[path] = self.fcn(path, fields, device)\n",
        "            return self.cache[path]\n",
        "        else:\n",
        "            return self.fcn(file, fields, self.device)\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "# Helper functions for keeping track of various measurements\n",
        "\n",
        "class accuracy_metric():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.total=0\n",
        "    self.correct=0\n",
        "    self.data = []\n",
        "\n",
        "  def add(self,outputs, labels):\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    self.total   += labels.size(0)\n",
        "    self.correct += (predicted == labels).sum().item()\n",
        "    \n",
        "  def value(self): \n",
        "    val = self.correct/self.total if self.total else 0\n",
        "    self.data.extend([val])\n",
        "    return val\n",
        "\n",
        "class running_mean():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.total = 0\n",
        "    self.count = 0\n",
        "  \n",
        "  def add(self,value):\n",
        "    self.total+=value\n",
        "    self.count+=1\n",
        "  \n",
        "  def value(self):\n",
        "    return self.total/self.count if self.count else 0\n",
        "#_______________________________________________________________________________\n",
        "# Helper functions for debugging\n",
        "\n",
        "import torch.nn as nn\n",
        "class Print(nn.Module):\n",
        "  def forward(self,x):\n",
        "    print(x.size())\n",
        "    return x\n",
        "\n",
        "def count_parameters(model):\n",
        "  print(\"num parameters = \",sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vutD_Sk3HbV7"
      },
      "source": [
        "**Activate Tensorboard, to keep track of our training runs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmBCjtgTJ_2Z"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "# delete old tensorboard log files, if necessary\n",
        "!rm -r runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKhtPgWJ39h0"
      },
      "source": [
        "%tensorboard --logdir runs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg_MloIKPudj"
      },
      "source": [
        "**Define our model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deSozHBEPbpk"
      },
      "source": [
        "import torch, torch.nn as nn\n",
        "\n",
        "def get_model():\n",
        "\n",
        "  nrow,ncol = 64, 64 \n",
        "  C = 3    # number channels in each conv2d\n",
        "  N = 10   # number neurons in fully connected layer\n",
        "  K = 3    # convolutional kernel size\n",
        "  P = K//2 # padding needed to keep the same shape\n",
        "  \n",
        "  Cin = len(train_data.inputs) # number of input channels \n",
        "  Nout= len(train_data.outputs)# number of output values\n",
        "\n",
        "  model = nn.Sequential(\n",
        "      nn.Conv2d(Cin, C,  kernel_size=K,  stride=1, padding=P), nn.MaxPool2d(2,2),nn.ReLU(), \n",
        "      nn.Conv2d(C  , C,  kernel_size=K,  stride=1, padding=P), nn.ReLU(), nn.MaxPool2d(2,2),\n",
        "      nn.Conv2d(C  , C,  kernel_size=K,  stride=1, padding=P), nn.ReLU(), nn.MaxPool2d(2,2),\n",
        "      nn.Conv2d(C  , C,  kernel_size=K,  stride=1, padding=P), nn.ReLU(), nn.MaxPool2d(2,2),\n",
        "      nn.Conv2d(C  , C,  kernel_size=K,  stride=1, padding=P), nn.ReLU(), nn.MaxPool2d(2,2),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(C*nrow*ncol//(4*4*4*4*4), N), nn.ReLU(),\n",
        "      nn.Linear(N, Nout)\n",
        "      )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V355CEe1rld"
      },
      "source": [
        "**Train a simple CNN to classify storms**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DDK6MJ9IEaZ"
      },
      "source": [
        "'''\n",
        "TODO: Some activities to try\n",
        "\n",
        "1. Measure training time: CPU           use_cache=False. runtime->type = None\n",
        "2. Measure training time: CPU + cache   use_cache=True.  runtime->type = None\n",
        "3. Measure training time: GPU           use_cache=False. runtime->type = GPU\n",
        "4. Measure training time: GPU + cache   use_cache=True.  runtime->type = GPU\n",
        "   Note: when you change runtime type, it deletes your files. So you have to re-download.\n",
        "   Use nepoch = 2\n",
        "\n",
        "   training time for epoch 0    = ??, ??, ??, ?? seconds\n",
        "   training time for epoch 1    = ??, ??, ??, ?? seconds\n",
        "   training time for 100 epochs = ??, ??, ??, ?? minutes (use: epoch0 + 99 * epoch1)\n",
        "\n",
        "   (from now on, always use GPU + cache)\n",
        "\n",
        "5. How accurate is our prediction (use nepochs = 30.)\n",
        "   val_accuracy = ??\n",
        "\n",
        "6. How important is the quantity of training data?\n",
        "          ftrain = 0.1   0.2   0.5   0.9\n",
        "   val_accuracy  = ??    ??    ??    ??\n",
        "\n",
        "7. How much can we improve the result by including wind and pressure data?\n",
        "   fields = [\"pwat\",\"u\",\"v\",\"mslp\"] val_accuracy = ??\n",
        "\n",
        "8. How well can we classify  10 categories, using water vapor alone? fields=['pwat']\n",
        "   Recall, a random guess should get you around 10%\n",
        "   categories = ['TD','TS','CAT1','CAT2','CAT3','CAT4','CAT5','DS','SS','NONE']\n",
        "   val_accuracy = ??\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SglbcFdueQOd"
      },
      "source": [
        "# build data cache.\n",
        "# you will need to re-run this, if you change fields or categories\n",
        "cache = Cache(load_netcdf, device, memory_cap=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD6RYubcipwg"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from time import time\n",
        "import nvidia_smi\n",
        "t_start = time()\n",
        "\n",
        "# USER PARAMETERS\n",
        "\n",
        "ftrain      = 0.90               # fraction of data to use for training\n",
        "use_cache   = True               # do we want to use the data cache?\n",
        "nepochs     = 2                  # number of epochs to train for\n",
        "fields      = [\"pwat\"]           # input variables\n",
        "categories  = ['TS','NONE']      # output variables\n",
        "\n",
        "# DATA\n",
        "\n",
        "path         = \"/content/crop_64\" # location of the data\n",
        "load_fcn     = cache.load if use_cache else load_netcdf\n",
        "\n",
        "train_data   = TropicalCycloneDataset(path, fields, categories, load_fcn, start=0.0, stop=ftrain)\n",
        "val_data     = TropicalCycloneDataset(path, fields, categories, load_fcn, start=ftrain, stop=1.0)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=200,  shuffle=True,  num_workers=0)\n",
        "val_loader   = DataLoader(val_data  , batch_size=200,  shuffle=False, num_workers=0)\n",
        "\n",
        "# MODEL\n",
        "\n",
        "model = get_model().to(device)\n",
        "count_parameters(model)\n",
        "\n",
        "# CONFIGURE\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "loss_fcn  = nn.CrossEntropyLoss()\n",
        "writer    = SummaryWriter()\n",
        "acc_list  = []\n",
        "\n",
        "for epoch in range(nepochs) :\n",
        "\n",
        "  # VALIDATE\n",
        "  \n",
        "  with torch.no_grad():\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = running_mean()\n",
        "    val_acc  = accuracy_metric()\n",
        "\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "      if(epoch==0): print(f\"epoch 0: validation batch {i+1}/{len(val_loader)}\")\n",
        "\n",
        "      inputs, labels = data[0].to(device), data[1].to(device)\n",
        "      outputs   = model(inputs)\n",
        "      loss      = loss_fcn(outputs,labels)\n",
        "      val_loss.add(loss.item())\n",
        "      val_acc.add(outputs,labels)\n",
        "      \n",
        "  # TRAIN\n",
        "\n",
        "  model.train()\n",
        "  train_acc  = accuracy_metric()\n",
        "  train_loss = running_mean()\n",
        "  gpu        = running_mean()\n",
        "  t0         = time()\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    if(epoch==0): print(f\"epoch 0: training batch {i+1}/{len(train_loader)}\")\n",
        "\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs = model(inputs)\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fcn(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss.add(loss.item())\n",
        "    train_acc.add(outputs,labels)\n",
        "    if(device.type ==\"cuda\"): gpu.add(nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu)\n",
        "\n",
        "  # DISPLAY PROGRESS\n",
        "\n",
        "  print(f\"epoch={epoch:3d} dt={time()-t0:.3f}s gpu ={gpu.value():.0f}% training loss={train_loss.value():.5f} val loss ={val_loss.value():.5f} train_acc={train_acc.value():.3f} val_acc={val_acc.value():.3f} \")\n",
        "  writer.add_scalar('Loss/val'      , val_loss.value()  , epoch)\n",
        "  writer.add_scalar('Loss/train'    , train_loss.value(), epoch)\n",
        "  writer.add_scalar('Accuracy/val'  , val_acc.value()   , epoch)\n",
        "  writer.add_scalar('Accuracy/train', train_acc.value() , epoch)\n",
        "  acc_list.extend( [val_acc.value()] )\n",
        "\n",
        "print()\n",
        "print(f\"total elapsed training time = { (time()-t_start)/60.0:,.1f} minutes\")\n",
        "print(f\"maximum validation accuracy = { max(acc_list) }\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-hh3y0ucJzR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}